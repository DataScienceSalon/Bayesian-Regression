---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Part 4: Modeling
This section has three purposes: first, explore BMA within the context of each of the nine default parameter priors under consideration; second, evaluate and compare the predictive accuracy of BMA, highest probability model (HPM), best predictive model (BPM), and the median probability model (MPM) for each of the default priors; and third, to reveal the parameter estimates, the predictive means and values of the best performing models under squared error.

## Bayesian Model Averaging
`r kfigr::figr(label = "model_spaces", prefix = TRUE, link = TRUE, type="Figure")` shows the model spaces for each of the nine parameter priors. The AIC prior stands out as having the largest model sizes (`r min(summary(models[[2]])[21,-1])` - `r max(summary(models[[2]])[21,-1])`) among its top five models. The remaining models look almost identical. Runtime, imdb_rating, and critics_score appear to be the variables most common among the remaining models.

**Model Spaces**
```{r model_spaces, fig.height = 4}
graphics::par(mfrow = c(1,3), family = "Open Sans")
for (i in 1:length(models)) {
  bmaImage(models[[i]], rotate = F, main = paste(models[[i]]$priorDesc, "Model Rank"))
  title(sub = "")
}
```
`r kfigr::figr(label = "model_spaces", prefix = TRUE, link = TRUE, type="Figure")`: Model spaces for under BMA for the nine candidate parameter priors: model prior = uniform

**Model Posterior Probabilities**
Of the posterior probabilities for the highest probability model under each prior shown in `r kfigr::figr(label = "model_post_probs", prefix = TRUE, link = TRUE, type="Figure")` the `r analysis$m1s$data[1,1]` prior yielded the model with the greatest probability, given the data. The `r analysis$m1s$data[1,1]` prior was followed by `r analysis$m1s$data[2,1]` and `r analysis$m1s$data[3,1]` priors. The `r mins <- analysis$m1s$data %>% filter(pctMean < 0)` `r mins[[1,1]]` and `r mins[[2,1]]` priors gave models with posterior probabilities `r abs(mins$pctMean[[1]])` and `r abs(mins$pctMean[[2]])` percent below the mean posterior model probability, respectively.

```{r model_post_probs}
analysis$m1s$plots$postProbs
```
`r kfigr::figr(label = "model_post_probs", prefix = TRUE, link = TRUE, type="Figure")`: Posterior probabilities for the top model under each prior: model prior = uniform 

```{r model_R2_size}
g <- list(analysis$m1s$plots$r2, analysis$m1s$plots$size)
do.call("grid.arrange", c(g, ncol = 2))
```
`r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")`: Model sizes and coefficients of determination $R^2$ values for the top model under each prior: model prior = uniform 

**Model Coefficients of Determination**
The coefficients of determination $R^2$ are displayed for each prior in `r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")` Naturally, the `analysis$m1s$data %>% filter(R2 == max(R2)) %>% select(Prior)` had the highest $R^2$ as this metric favors large models. However, the larger model didn't produce an $R^2$ significantly greater than the mean of `r round(mean(analysis$m1s$data$R2), 2)`.

**Model Size**
As shown in `r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")`, the AIC model was significantly larger than the mean model size of `r size <- analysis$m1s$data %>% filter(Size != max(Size)) %>% select(Size)` `r round(mean(size$Size), 2)` parameters.

### Model Performance 

`r kfigr::figr(label = "performance_table", prefix = TRUE, link = TRUE, type="Table")`: Model sizes and coefficients of determination $R^2$ values for the top model under each prior: model prior = uniform 
```{r performance_table}
knitr::kable(report$table, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```



* * *
