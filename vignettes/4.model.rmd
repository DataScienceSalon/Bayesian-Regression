# Part 4: Modeling
This section has three purposes: first, explore BMA within the context of each of the nine default parameter priors under consideration; second, evaluate and compare the predictive accuracy of BMA, highest probability model (HPM), best predictive model (BPM), and the median probability model (MPM) for each of the default priors; and third, to diagnose and reveal the parameter estimates, the predictive means and values of the best performing models under squared error. BMA was performed on the preprocesed movie data set, containing `r nrow(yX)` observations. Predictive accuracy was measured against  hold-out data sets. Once the best performing models were identified, parameter estimates and predictive intervals were obtained using the full data set. 

## Bayesian Model Averaging
`r kfigr::figr(label = "model_spaces", prefix = TRUE, link = TRUE, type="Figure")` shows the model spaces for each of the nine parameter priors. The AIC prior stands out as having the largest model sizes (`r min(summary(models[[2]])[21,-1])` - `r max(summary(models[[2]])[21,-1])`) among its top five models. Zellner's g-prior models average between `r min(summary(models[[5]])[21,-1])` - `r max(summary(models[[5]])[21,-1])` parameters for its top five models. The remaining models were almost identical. Runtime, imdb_rating, and critics_score appear to be the variables most common among the remaining models.

**Model Spaces**   
```{r model_spaces, fig.height = 4}
graphics::par(mfrow = c(1,3), family = "Open Sans")
for (i in 1:length(models)) {
  bmaImage(models[[i]], rotate = F, main = paste(models[[i]]$priorDesc, "Model Rank"))
  title(sub = "")
}
```
`r kfigr::figr(label = "model_spaces", prefix = TRUE, link = TRUE, type="Figure")`: Model spaces for under BMA for the nine candidate parameter priors: model prior = uniform

**Parameter Inclusion Probabilities**
`r kfigr::figr(label = "pip", prefix = TRUE, link = TRUE, type="Table")` reports the BMA posterior inclusion probabilities for all nine prior distributions applied to the complete movie dataset. Posterior inclusion probabilities and the number of predictors that show evidence of an effect on audience scores vary substantially across priors. The number of predictors with inclusion probability exceeding 50%, (not including the intercept) ranges from a low of `r min(analysis$pip$nSig)` (BIC, Hyper G-N, and Zellner-Siow priors) to a high of `r max(analysis$pip$nSig)` regressors for the AIC prior. IMDb rating and critics score the parameters showing the most evidence of a significant effect on audience scores across priors. Runtime, a curious associate of audience scores, is evidenced to have a significant effect on the dependent variable for the AIC, Empirical Bayes (Global) and Zellner's g priors. MPAA R rating indicator and the log of IMDb votes also show evidence of a significant effect with AIC and Zellner's g priors.

`r kfigr::figr(label = "pip", prefix = TRUE, link = TRUE, type="Table")`: Posterior inclusion probabilities across parameter priors: model prior = uniform
```{r pip}
knitr::kable(analysis$pip$plots$table, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = "Posterior inclusion probabilities that exceed 50% are in bold font (Jeffreys, 1961).")
```

**Model Posterior Probabilities**  
Of the posterior probabilities for the highest probability model under each prior shown in `r kfigr::figr(label = "model_post_probs", prefix = TRUE, link = TRUE, type="Figure")` the `r analysis$m1s$data[1,1]` prior yielded the model with the greatest probability, given the data. The `r analysis$m1s$data[1,1]` prior was followed by `r analysis$m1s$data[2,1]` and `r analysis$m1s$data[3,1]` priors. The `r mins <- analysis$m1s$data %>% filter(pctMean < 0)` `r mins[[1,1]]` and `r mins[[2,1]]` priors gave models with posterior probabilities `r abs(mins$pctMean[[1]])` and `r abs(mins$pctMean[[2]])` percent below the mean posterior model probability, respectively.

```{r model_post_probs}
analysis$m1s$plots$postProbs
```
`r kfigr::figr(label = "model_post_probs", prefix = TRUE, link = TRUE, type="Figure")`: Posterior probabilities for the top model under each prior: model prior = uniform 

```{r model_R2_size}
g <- list(analysis$m1s$plots$r2, analysis$m1s$plots$size)
do.call("grid.arrange", c(g, ncol = 2))
```
`r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")`: Model sizes and coefficients of determination $R^2$ values for the top model under each prior: model prior = uniform 

**Model Coefficients of Determination**   
The coefficients of determination $R^2$ are displayed for each prior in `r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")` Naturally, the `r analysis$m1s$data %>% filter(R2 == max(R2)) %>% select(Prior)` had the highest $R^2$ as this metric favors large models. However, this larger model with its 12 predictors didn't produce an $R^2$ significantly greater than the mean of `r round(mean(analysis$m1s$data$R2), 2)`.

**Model Size**
As shown in `r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")`, the AIC model was significantly larger than the mean model size of `r size <- analysis$m1s$data %>% filter(Size != max(Size)) %>% select(Size)` `r round(mean(size$Size), 2)` parameters.

## Model Performance 
Now, the predictive performance of competing default priors are compared on the basis of mean squared error (MSE) on hold-out samples. Predictions were rendered using the BAS package [@Clyde2017], for the BMA, best predictive model (BPM), the highest probability model (HPM), and the median probability model (MPM) estimators on all nine priors simultaneously. The MSE of prediction is:
$$1/n_{test}\displaystyle\sum_{y_{new}\in D_{test}}(y_{new} - \hat{y}_{new})^2$$
where $n_{test}$ is the number of observations in $D_{test}$.

The movie data set was randomly split into a training set, $D_{train}$, (80% of observations) which was used to estimate the BMA predictive distribution, and a test set, $D_{test}$, (20% of observations) which was used to assess the quality of the resulting predictive distributions. The mean MSE was computed for each prior and this analysis was repeated 400 times for 400 different random splits, averaging the MSE over splits for each prior and estimator. `r kfigr::figr(label = "performance_table", prefix = TRUE, link = TRUE, type="Table")` shows the predictive performance of the nine parameter priors and four estimators, for a total of 36 predictions, in conjunction with the uniform model priors a evaluated by the MSE.

`r kfigr::figr(label = "performance_table", prefix = TRUE, link = TRUE, type="Table")`: Parameter priors and predictive performance by prior and estimator (movie dataset, model prior: uniform); 100 subsamples 
```{r performance_table}
knitr::kable(report$table, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = "The five lowest MSE scores are in bold font. The lowest MSE is in red font.")
```

The `r report$best$PriorDesc[1]` prior using the `r report$best$Estimator[1]` estimator outperformed the other 35 models, decisively. To ascertain the significance of the differences in mean MSE among the models, t-tests of the MSE means were conducted at an $\alpha = .05$ significance level. `r kfigr::figr(label = "performance_compare", prefix = TRUE, link = TRUE, type="Table")` reports the top four models in order of ascending mean MSE and the p.value for the probability that $\mu_m = \mu_{best}$, where $\mu_m$ is the mean MSE for model $m$ and $\mu_{best}$ is the lowest MSE among all models and estimators. As indicated by the p.values, the differences in mean MSE were significant; however, `r kfigr::figr(label = "top5", prefix = TRUE, link = TRUE, type="Figure")`, reveals significant overlap in the distributions of MSE, suggesting that other models should be given some consideration.

`r kfigr::figr(label = "performance_compare", prefix = TRUE, link = TRUE, type="Table")`: Mean MSE by prior and estimator for top four models with p.values for the probability that $\mu_m = \mu_{best}$, where $\mu_m$ is the mean MSE for model $m$ and $\mu_{best}$ is the lowest MSE among all models and estimators. 
```{r performance_compare}
scores <- report$best[c(1:4),c(2,3,4,5)]
colnames(scores) <- c("Prior", "Estimator", "Mean MSE", "p.value")
knitr::kable(scores, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```

```{r top5}
report$plots$box
```
`r kfigr::figr(label = "top5", prefix = TRUE, link = TRUE, type="Figure")`: Distribution of MSE among top 4 models

## Model Evaluation
The model selected for final prediction on 2016 data would be that which performed well on squared error while meeting Bayesian regression inference conditions, specifically residual versus predicted homoscedacticity. Here, the top four models are evaluated in terms of residual versus fitted distributions, and the parameter coefficients. As shown in `r kfigr::figr(label = "rvf", prefix = TRUE, link = TRUE, type="Figure")`, all four priors generated similar distributions of residuals vis-a-vis fitted values. Residual concentration tended to align with the proximity of fitted values. The same three observations were influential and potential outliers across all priors.   

**Residuals vs Fitted**
```{r rvf, fig.height = 12}
par(mfrow = c(3, 3))
for (i in 1:length(models)) plot(models[[i]], which = 1, caption = (paste0("Residuals vs Fitted (", models[[i]]$prior,")")))
```
`r kfigr::figr(label = "rvf", prefix = TRUE, link = TRUE, type="Figure")`: Residual vs Fitted Values


**Parameter Estimates**
```{r pdc, fig.height = 12}
do.call("grid.arrange", c(eval$pdc, ncol = 2))
```
`r kfigr::figr(label = "pdc", prefix = TRUE, link = TRUE, type="Figure")`: Coefficient Estimates

* * *
