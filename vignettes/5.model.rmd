# Modeling
Here the principle question of this paper. the effect of default priors on Bayesian regression model selection, model size, posterior inclusion probabilities of regressors, and on predictive performance, is investigated in four steps. First,  explore **BMA** within the context of each of the nine default parameter priors under consideration. Second, evaluate **model performance**  in terms of **predictive accuracy** of the BMA, highest probability, best predictive and median predictive models for each of the default priors. Lastly, examine the parameter estimates for the models which performed **best** on a squared error basis. The best of the models would be used to predict audience scores for a heretofore, unobserved set of feature films released in 2016.

## Bayesian Model Averaging
Bayesian model averaging was conducted on the full data of `r nrow(yX)` observations.

### Model Spaces
`r kfigr::figr(label = "model_spaces", prefix = TRUE, link = TRUE, type="Figure")` shows the model spaces for each of the nine default parameter priors. The rows correspond to each of the variables and the intercept term, the names of which are indicated along the y-axis.  The columns pertain to the models used in the averaging process, whereby the width of the columns represent the posterior  probabilities of the models. A wider column indicates a higher posterior probability, which is indicated along the x-axis.  The models are sorted from the left, models with the highest posterior probability, to the left. Variables excluded from the model are shown in black for each column. The colored areas represent variables included in the model, whereby the color is associated with, and proportional to, the log posterior probability of the variable. Models that have the same color have similar log probabilities.

The AIC prior stands out as having the largest model sizes (`r min(summary(models[[2]])[21,-1])` - `r max(summary(models[[2]])[21,-1])`) among its top five models. Zellner's g-prior models average between `r min(summary(models[[5]])[21,-1])` - `r max(summary(models[[5]])[21,-1])` parameters for its top five models. The remaining models were almost identical. Runtime, imdb_rating, and critics_score were most common among the remaining models.

```{r model_spaces, fig.height = 4}
graphics::par(mfrow = c(1,3), family = "Open Sans")
for (i in 1:length(models)) {
  bmaImage(models[[i]], rotate = F, main = paste(models[[i]]$priorDesc, "Model Rank"))
  title(sub = "")
}
```
`r kfigr::figr(label = "model_spaces", prefix = TRUE, link = TRUE, type="Figure")`: Model spaces for under BMA for the nine candidate parameter priors: model prior = uniform

### Parameter Inclusion Probabilities
`r kfigr::figr(label = "pip", prefix = TRUE, link = TRUE, type="Table")` reports the BMA posterior inclusion probabilities for all nine prior distributions applied to the complete movie dataset. Posterior inclusion probabilities and the number of predictors that show evidence of an effect on audience scores vary substantially across priors. The number of predictors with inclusion probability exceeding 50%, (not including the intercept) ranges from a low of `r min(analysis$pip$nSig)` (BIC, Hyper G-N, and Zellner-Siow priors) to a high of `r max(analysis$pip$nSig)` regressors for the AIC prior. IMDb rating and critics score the parameters showing the most evidence of a significant effect on audience scores across priors. Runtime, a curious associate of audience scores, is evidenced to have a significant effect on the dependent variable for the AIC, Empirical Bayes (Global) and Zellner's g priors. MPAA R rating indicator and the log of IMDb votes also show evidence of a significant effect with AIC and Zellner's g priors.

`r kfigr::figr(label = "pip", prefix = TRUE, link = TRUE, type="Table")`: Posterior inclusion probabilities across parameter priors: model prior = uniform
```{r pip}
knitr::kable(analysis$pip$plots$table, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = "Posterior inclusion probabilities that exceed 50% are in bold font (Jeffreys, 1961).")
```

### Model Posterior Probabilities
Of the posterior probabilities for the highest probability model under each prior shown in `r kfigr::figr(label = "model_post_probs", prefix = TRUE, link = TRUE, type="Figure")` the `r analysis$m1s$data[1,1]` prior yielded the model with the greatest probability, given the data. The `r analysis$m1s$data[1,1]` prior was followed by `r analysis$m1s$data[2,1]` and `r analysis$m1s$data[3,1]` priors. The `r mins <- analysis$m1s$data %>% filter(pctMean < 0)` `r mins[[1,1]]` and `r mins[[2,1]]` priors gave models with posterior probabilities `r abs(mins$pctMean[[1]])` and `r abs(mins$pctMean[[2]])` percent below the mean posterior model probability, respectively.

```{r model_post_probs}
analysis$m1s$plots$postProbs
```
`r kfigr::figr(label = "model_post_probs", prefix = TRUE, link = TRUE, type="Figure")`: Posterior probabilities for the top model under each prior: model prior = uniform 

```{r model_R2_size}
g <- list(analysis$m1s$plots$r2, analysis$m1s$plots$size)
do.call("grid.arrange", c(g, ncol = 2))
```
`r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")`: Model sizes and coefficients of determination $R^2$ values for the top model under each prior: model prior = uniform 

### Model Coefficients of Determination
The coefficients of determination $R^2$ are displayed for each prior in `r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")` Naturally, the `r analysis$m1s$data %>% filter(R2 == max(R2)) %>% select(Prior)` had the highest $R^2$ as this metric favors large models. However, this larger model with its 12 predictors didn't produce an $R^2$ significantly greater than the mean of `r round(mean(analysis$m1s$data$R2), 2)`.

### Model Size
As shown in `r kfigr::figr(label = "model_R2_size", prefix = TRUE, link = TRUE, type="Figure")`, the AIC model was significantly larger than the mean model size of `r size <- analysis$m1s$data %>% filter(Size != max(Size)) %>% select(Size)` `r round(mean(size$Size), 2)` parameters.

## Model Performance 
Now, the predictive performance of competing default priors are compared on the basis of mean squared error (MSE) on hold-out samples. Predictions were rendered using the BAS package [@Clyde2017], for four model estimators: (1) the BMA model, (2) the best predictive model (BPM), (3) the highest probability model (HPM), and (4) the median probability model (MPM) estimators. Predictions for each of the nine priors and the four model estimators were rendered for a total of 36 predictions. The movie data set was randomly split into a training set, $D_{train}$, (80% of observations) which was used to train the model, and a test set, $D_{test}$, (20% of observations) which was used to assess the quality of the resulting predictive distributions. The mean MSE was computed for each prior and estimator, for a total of 36 models.  This analysis was repeated for the 36 models, 400 times for 400 different random splits. The average MSE of predictions for each model was computed as follows:
$$1/j * 1/n_{test}\displaystyle\sum_{j = 1}^{t}\displaystyle\sum_{i = 1}^{n_{test}^{t}}(y_{test} - \hat{y}_{test})^2$$
where:    
$t$ is the total number of trials  
$n_{test}^{t}$ is the total number of observations in in $D_{test}^j$ 
$y_{test}$ is the observed audience score for an observation in $D_{test}^j$   
$\hat{y}_{test}$ is the *predicted* audience score for an observation in $D_{test}$   


`r kfigr::figr(label = "performance_table", prefix = TRUE, link = TRUE, type="Table")` shows the predictive performance of the nine parameter priors and four estimators, in conjunction with the uniform model priors a evaluated by the MSE.

`r kfigr::figr(label = "performance_table", prefix = TRUE, link = TRUE, type="Table")`: Parameter priors and predictive performance by prior and estimator (movie dataset, model prior: uniform); 100 subsamples 
```{r performance_table}
knitr::kable(report$table, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = "The five lowest MSE scores are in bold font. The lowest MSE is in red font.")
```

The top five scores are highlighted in **bold** and the lowest average MSE is noted in red font. The `r report$best$PriorDesc[1]` prior using the `r report$best$Estimator[1]` estimator outperformed the other 35 models; however not decisively. To ascertain the significance of the differences in mean MSE among the models, t-tests of the MSE means were conducted at an $\alpha = .05$ significance level. `r kfigr::figr(label = "performance_compare", prefix = TRUE, link = TRUE, type="Table")` reports the top four models in order of ascending mean MSE and the p.value for the probability that $\mu_m = \mu_{best}$, where $\mu_m$ is the mean MSE for model $m$ and $\mu_{best}$ is the lowest MSE among all models and estimators. As indicated by the p.values, the differences in mean MSE were not significant. Further, the boxplot of the distribution of MSE scores in `r kfigr::figr(label = "top5", prefix = TRUE, link = TRUE, type="Figure")`, reveals significant overlap in the distributions of MSE. Though other experiments showed differences which were significant, this plot supports the notion that the each of the four models should be given some consideration.

`r kfigr::figr(label = "performance_compare", prefix = TRUE, link = TRUE, type="Table")`: Mean MSE by prior and estimator for top four models with p.values for the probability that $\mu_m = \mu_{best}$, where $\mu_m$ is the mean MSE for model $m$ and $\mu_{best}$ is the lowest MSE among all models and estimators. 
```{r performance_compare}
scores <- report$best[c(1:4),c(2,3,4,5)]
colnames(scores) <- c("Prior", "Estimator", "Mean MSE", "p.value")
knitr::kable(scores, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```

```{r top5}
report$plots$box
```
`r kfigr::figr(label = "top5", prefix = TRUE, link = TRUE, type="Figure")`: Distribution of MSE among top 4 models

## Parameter Estimates
As indicated in `r kfigr::figr(label = "pdc", prefix = TRUE, link = TRUE, type="Table")`, there was some variation among the selected predictors as well as their estimates.  Markedly, all four models produced precisely the same estimate for the intercept, although with slighly different levels of uncertainty. The single greatest positive influencer of audience scores, **IMDb ratings** would generate between 14-15 audience score points for each point on the IMDb rating scale. All estimates had similar variations with $SD \approx 0.58$. Like IMDB rating, **critics score** was a positive factor in all four models, but with far less influence on audience scores than was IMDb rating. A point on the critics score scale would result in an estimated 0.7 points on the audience score scale. There was a slight negative association between **runtimes** and audience scores (approximate 0.05 point drop for each minute of run time) in the AIC (MPM and the g-prior (MPM) models. Runtime wasn't a factor in the EB-Global (MPM) and Hyper-G-Laplace (MPM) models. An **MPAA R** rating wasn't necessarily good for a film's audience score. The AIC (MPM) and g-prior (MPM) models associated an R rating with a drop of approximately 1.5 points in the audience score. The **Oscar performance** of the films were relatively significant predictors in the AIC(MPM) model, but the devil is in the details.  Only **Best Picture nominations** were positively associated with audience scores, with a relatively high estimate of 5 points; however, individual performance was not rewarded. Best actor and best actress winners were negatively associated with audience scores. It would appear that audiences prefer ensembles.

`r kfigr::figr(label = "pdc", prefix = TRUE, link = TRUE, type="Table")`: Coefficient Estimates by Prior and Estimator
```{r pdc, results='asis'}
n <- names(eval$pdc)
for (i in 1:length(eval$pdc)) {
  print(knitr::kable(eval$pdc[[i]], digits = 4) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::group_rows(n[i], 1,nrow(eval$pdc[[i]])))
}
```

## Top Model Evaluations
Having explored prediction accuracy, and characterized the parameter estimates in terms of their effects on audience score, the focus turned to determining the quality of those model inferences. Specifically, the distribution of errors vis-a-vis predictions were evaluated for homoscedacticity. `r kfigr::figr(label = "rvf", prefix = TRUE, link = TRUE, type="Figure")`, presented very similar distributions of errors vis-a-vis fitted values for each of the nine priors. A greater concentration of residuals was observed at the higher audience scores; whereas the residuals for lower scores were more dispersed. Also, the quality of the predictions tended to be positively associated with the quantity of observations at any particular range of audience scores. 

### Residuals vs Fitted
```{r rvf, fig.height = 12}
par(mfrow = c(3, 3))
for (i in 1:length(models)) plot(models[[i]], which = 1, caption = (paste0("Residuals vs Fitted (", models[[i]]$prior,")")))
```
`r kfigr::figr(label = "rvf", prefix = TRUE, link = TRUE, type="Figure")`: Residual vs Fitted Values

Though residuals appeared to center at zero, the complete lack of homoscedacticity among the models could not be ignored. As such, the quality and reliability of the above inferences that depend upon $\epsilon \sim N(0, \sigma^2)$ are suspect.  That said, the overarching objective was to explore relative *prediction accuracy* of various default parameter priors and estimators. It was therefore concluded that inference using any of the nine priors and four estimators would be suspect; however, the 36 models were on "equal footing" with respect to prediction. Hence all four models would advance to the prediction phase.


* * *
