# Models
The principle question of this paper - the effect of default priors on Bayesian regression model selection, model size, posterior inclusion probabilities of regressors, inference and on predictive performance - was investigated in four steps. First, the model averages under **BMA** were explored for each of the nine default parameter priors under consideration. Two sets of models, based upon two different data sets, were fit.  Model set Alpha was based upon the full data set and model set Beta was developed using the full data set without the imdb_rating variable. Model set Beta, as will be shown in the next step, produced less heteroskedactic errors, and therefore, would be the preferred model for inference. Second, **model assessments** were conducted to reveal the extent to which model assumptions for inference were met. Third, **model performance**  of the BMA, highest probability models (HPM), best predictive models (BPM) and the median predictive models (MPM) was evaluated for both model sets. Mean squared error was the metric used for evaluating model performance. Fourth, the **parameter estimates** for the top models in model sets Alpha and Beta were interpreted and lastly, the best of the models would be used to predict audience scores for a heretofore, unobserved set of feature films released in 2016.

## Bayesian Model Averaging
Bayesian model averaging was conducted for both model sets Alpha and Beta.  Both model sets were based upon the full data of `r nrow(yX)` observations with a log transformation of the imdb_num_votes variable.  Model set Beta was fit without the imdb_rating variable. 

### Model Spaces
The following figures show the model spaces for each model set and nine default parameter priors. The rows correspond to each of the variables and the intercept term, the names of which are indicated along the y-axis.  The columns pertain to the models used in the averaging process, whereby the width of the columns represent the posterior  probabilities of the models. A wider column indicates a higher posterior probability, the values of which are indicated along the x-axis.  The models are sorted left to right, from highest posterior probability to lowest. Variables excluded from the model are shown in black for each column. The colored areas represent variables included in the model, whereby the color is associated with, and proportional to, the log posterior probability of the variable. Models that have the same color have similar log probabilities.

#### Model Set Alpha

The largest models were those based upon the AIC prior, with model sizes ranging from `r min(summary(models[[2]])[21,-1])` to `r max(summary(models[[2]])[21,-1])` regressors for the top five models. The top five Zellner's g-prior models included between `r min(summary(models[[5]])[21,-1])` and `r max(summary(models[[5]])[21,-1])` parameters. The remaining model spaces, containing between `r min(summary(models[[4]])[21,-1])` to `r max(summary(models[[4]])[21,-1])` predictors, were almost identical.

```{r model_spaces, fig.height = 4}
graphics::par(mfrow = c(1,3), family = "Open Sans")
for (i in 1:length(models)) {
  bmaImage(models[[i]], rotate = F, main = paste(models[[i]]$priorDesc, "Model Rank"))
  title(sub = "")
}
```
`r kfigr::figr(label = "model_spaces", prefix = TRUE, link = TRUE, type="Figure")`: Model spaces under BMA for the nine candidate parameter priors: model prior = uniform

### Model Complexity
The relationships between model complexity and posterior probability (`r kfigr::figr(label = "complexity", prefix = TRUE, link = TRUE, type="Figure")`) align with, and to an extent, explain the model spaces above.  Recall, the AIC and g-prior models were characterized by their larger sizes relative to the those of the other models.  Similarly, posterior probabilities for the AIC and g-prior models tended to occur at the higher dimensions. On the other hand, the maximum posterior probabilities for the other priors tended to occur with models containing between three and four predictors. 

```{r complexity, fig.height=12}
do.call("grid.arrange", c(analysis$complexity, ncol = 3))
```
`r kfigr::figr(label = "complexity", prefix = TRUE, link = TRUE, type="Figure")`: Model posterior probabilities vis-a-vis complexity

### Parameter Posterior Inclusion Probabilities under BMA
`r kfigr::figr(label = "pip", prefix = TRUE, link = TRUE, type="Table")` reports the BMA posterior inclusion probabilities for all nine prior distributions. The number of predictors with an inclusion probability exceeding 50%, (not including the intercept) ranged from a low of `r min(analysis$pip$nSig)` (BIC, EB-Local, Hyper-g, Hyper g/n, Hyper-g Laplace and Zellner-Siow priors) to a high of `r max(analysis$pip$nSig)` regressors for the AIC prior. 

`r kfigr::figr(label = "pip", prefix = TRUE, link = TRUE, type="Table")`: Posterior inclusion probabilities across parameter priors: model prior = uniform
```{r pip}
knitr::kable(analysis$pip$plots$table, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = "Posterior inclusion probabilities that exceed 50% are in bold font.")
```

With respect to the relative importance of the predictors, `r kfigr::figr(label = "mean_pip", prefix = TRUE, link = TRUE, type="Figure")` shows the average posterior inclusion probability across all priors for each parameter. IMDb ratings, critics score, and runtime had average inclusion probabilities exceeding 50% across all priors. 

```{r mean_pip}
analysis$pip$plots$meanPIP
```
`r kfigr::figr(label = "mean_pip", prefix = TRUE, link = TRUE, type="Figure")`: Mean posterior inclusion probabilities across all nine priors.


## Model Performance 
The predictive performance of competing default priors were compared on the basis of mean squared error (MSE) on hold-out samples. Predictions were rendered using the BAS package [@Clyde2017], for four model estimators: (1) the BMA model, (2) the best predictive model (BPM), (3) the highest probability model (HPM), and (4) the median probability model (MPM). Predictions for each of the nine priors and the four model estimators were rendered for a total of 36 predictions. The movie data set was randomly split into a training set, $D_{train}$, (80% of observations) which was used to train the model, and a test set, $D_{test}$, (20% of observations) which was used to assess the quality of the resulting predictive distributions. The mean MSE was computed for each prior and estimator, for a total of 36 models.  This analysis was repeated `r trials` times for `r trials` different random splits. The average MSE of predictions for each model was computed as follows:
$$1/j * 1/n_{test}\displaystyle\sum_{j = 1}^{t}\displaystyle\sum_{i = 1}^{n_{test}^{t}}(y_{test} - \hat{y}_{test})^2$$
where:    
$t$ is the total number of trials    
$n_{test}^{t}$ is the total number of observations in $D_{test}^j$     
$y_{test}$ is the observed audience score for an observation in $D_{test}^j$      
$\hat{y}_{test}$ is the *predicted* audience score for an observation in $D_{test}$      


The predictive performance of the nine parameter priors and four estimators, in conjunction with the uniform model priors and evaluated by MSE, are shown in `r kfigr::figr(label = "performance_table", prefix = TRUE, link = TRUE, type="Table")`. The top five scores (lowest average MSE) are highlighted in **bold** and the lowest average MSE is noted in red font. The `r report$best$PriorDesc[1]` prior using the `r report$best$Estimator[1]` estimator decisively outperformed the other 35 models.

`r kfigr::figr(label = "performance_table", prefix = TRUE, link = TRUE, type="Table")`: Parameter priors and predictive performance by prior and estimator (movie dataset, model prior: uniform); `r trials` subsamples 
```{r performance_table}
knitr::kable(report$table, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = "The five lowest MSE scores are in bold font. The lowest MSE is in red font.")
```

To ascertain the significance of the differences in mean MSE among the models, t-tests of the MSE means were conducted at an $\alpha = .05$ significance level. `r kfigr::figr(label = "performance_compare", prefix = TRUE, link = TRUE, type="Table")` reports the top ten models in order of ascending mean MSE. The p.values indicate the probability of observing a difference between the mean MSE for each model and the lowest MSE, under the null hypothesis. As indicated by the p.values, their was no statistically significant difference among the top nine models. 

`r kfigr::figr(label = "performance_compare", prefix = TRUE, link = TRUE, type="Table")`: Mean MSE by prior and estimator for top ten models with p.values for the $MSE -MSE_{best} = 0$. 
```{r performance_compare}
scores <- report$best[c(1:10),c(2,3,4,5)]
colnames(scores) <- c("Prior", "Estimator", "Mean MSE", "p.value")
knitr::kable(scores, escape = F, digits = 2) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```

Moreover, the box plot of the distribution of MSE scores in `r kfigr::figr(label = "top5", prefix = TRUE, link = TRUE, type="Figure")`, reveals almost identical distributions of MSE. The data 

```{r top5}
report$plots$box
```
`r kfigr::figr(label = "top5", prefix = TRUE, link = TRUE, type="Figure")`: Distribution of MSE among top 4 models

## Parameter Estimates
```{r}
n <- names(eval$pdc)
```
The model parameters for the top four models ranged from `r nrow(eval$pdc[[4]])` predictors for the `r n[4]` prior to `r nrow(eval$pdc[[2]])` regressors for the `r n[2]` prior. Despite significant variation in model complexity, similarities were extant among certain influential parameters. 

### `r n[1]` Prior
As shown in `r kfigr::figr(label = "pdc1", prefix = TRUE, link = TRUE, type="Table")`, the `r n[1]` prior produced a model `r nrow(eval$pdc[[1]])` regressors, where the IMDb rating had by far the greatest influence on audience scores. A one point increase in IMDb rating increases audience scores by approximately `r round(eval$pdc[[1]]$Mean[7],1)`points, all else equal. The next most influential variable was the film type. Audience scores for feature films were predicted to be approximately `r abs(round(eval$pdc[[1]]$Mean[2],1))` points lower than those for other film types. Success at the Academy also had a significant effect on audience scores, but the devil is in the details. Films nominated for Best Picture were associated with audience scores about `r abs(round(eval$pdc[[1]]$Mean[9],1))` points higher than other films, all else held equal. On the other hand, films earning Best Actor or Best Actress awards were associated with a reduction in audience scores of `r abs(round(eval$pdc[[1]]$Mean[10],1))` and `r abs(round(eval$pdc[[1]]$Mean[11],1))` points respectively, all else equal. This may be a consequence of collinearity among the Academy variables, or it could represent the audiences' preference for team work over individual achievement. Critics score, somewhat surprisingly, had very little influence on audience scores in this model. This was likely due to collinearity with IMDb ratings. 

`r kfigr::figr(label = "pdc1", prefix = TRUE, link = TRUE, type="Table")`: Coefficient Estimates for `r n[1]` prior.
```{r pdc1, results='asis'}
print(knitr::kable(eval$pdc[[1]], digits = 4) %>%
kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
kableExtra::group_rows(n[1], 1,nrow(eval$pdc[[1]])))

```


### `r n[2]` Prior
As shown in `r kfigr::figr(label = "pdc2", prefix = TRUE, link = TRUE, type="Table")`, the `r n[2]` and `r n[1]` priors were very similar in some important respects. Both models shared the top three most influential variables, IMDb rating, film type, and Best Picture nomination. In fact, the magnitude and direction of their effects on audience scores were congruent. The estimates for IMDb rating were `r round(eval$pdc[[1]]$Mean[7],1)` and `r round(eval$pdc[[2]]$Mean[9],1)` for the `r n[1]` and `r n[2]` models, respectively. Feature films were negatively associated with audience scores in both models. Estimates were `r round(eval$pdc[[1]]$Mean[2],1)` and `r round(eval$pdc[[2]]$Mean[2],1)` for the `r n[1]` and `r n[2]` models, respectively. Films nominated for Best Picture also did well in this model, boosting audience scores by approximately `r round(eval$pdc[[2]]$Mean[11],1)` points. As in the `r n[1]` prior, individual achievement at the Oscars was negatively associated with audience scores. Again, this may be a consequence of collinearity. Other variables with positive influence on audience scores included the critics scores, the drama genre, the log number of IMDB votes, summer season release, and top 200 box office achievement. Runtime, R-rating, Oscar season release each had a slight negative influence on audience scores.

`r kfigr::figr(label = "pdc2", prefix = TRUE, link = TRUE, type="Table")`: Coefficient Estimates for `r n[2]` prior.
```{r pdc2, results='asis'}
print(knitr::kable(eval$pdc[[2]], digits = 4) %>%
kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
kableExtra::group_rows(n[2], 1,nrow(eval$pdc[[2]])))

```

### `r n[3]` Prior
Again, IMDb rating was the dominated influence on audience scores for the `r n[3]` prior model. As indicated in `r kfigr::figr(label = "pdc3", prefix = TRUE, link = TRUE, type="Table")`, the parameter estimate for IMDb rating was `r round(eval$pdc[[3]]$Mean[5],1)`, slightly higher than those of the `r n[1]` and `r n[2]` priors. The log number of IMDb votes had a slight influence on audience scores; whereas a one percent increase in the number of IMDb votes, resulted in a `r round(eval$pdc[[3]]$Mean[7],1)` percent increase in audience score.

`r kfigr::figr(label = "pdc3", prefix = TRUE, link = TRUE, type="Table")`: Coefficient Estimates for `r n[3]` prior.
```{r pdc3, results='asis'}
print(knitr::kable(eval$pdc[[3]], digits = 4) %>%
kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
kableExtra::group_rows(n[3], 1,nrow(eval$pdc[[3]])))

```

### `r n[4]` Prior
The model depicted `r kfigr::figr(label = "pdc4", prefix = TRUE, link = TRUE, type="Table")` could be characterized as the most parsimonious of the top performing models. Audience scores were entirely a function of IMDb rating and critics score.

`r kfigr::figr(label = "pdc4", prefix = TRUE, link = TRUE, type="Table")`: Coefficient Estimates for `r n[4]` prior.
```{r pdc4, results='asis'}
print(knitr::kable(eval$pdc[[4]], digits = 4) %>%
kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center") %>%
kableExtra::group_rows(n[4], 1,nrow(eval$pdc[[4]])))

```

## Top Model Evaluations
Having explored prediction accuracy, and characterized the parameter estimates in terms of their effects on audience score, the focus turned to determining the quality of those model inferences. Specifically, the distribution of errors vis-a-vis predictions were evaluated for homoscedacticity. `r kfigr::figr(label = "rvf", prefix = TRUE, link = TRUE, type="Figure")`, presented very similar distributions of errors vis-a-vis fitted values for each of the nine priors. A greater concentration of residuals was observed at the higher audience scores; whereas the residuals for lower scores were more dispersed. The quality of the predictions tended to be positively associated with the quantity of observations at any particular range of audience scores. 

### Residuals vs Fitted
```{r rvf, fig.height = 12}
par(mfrow = c(3, 3))
for (i in 1:length(models)) plot(models[[i]], which = 1, caption = (paste0("Residuals vs Fitted (", models[[i]]$prior,")")))
```
`r kfigr::figr(label = "rvf", prefix = TRUE, link = TRUE, type="Figure")`: Residual vs Fitted Values

Though residuals appeared to center at zero, the complete lack of homoscedacticity among the models could not be ignored. As such, the quality and reliability of the above inferences that depend upon $\epsilon \sim N(0, \sigma^2)$ are suspect.  That said, the overarching objective was to explore relative *prediction accuracy* of various default parameter priors and estimators. It was therefore concluded that inference using any of the nine priors and four estimators would be suspect; however, the 36 models were on "equal footing" with respect to prediction. Hence all four models would advance to the prediction phase.


* * *
